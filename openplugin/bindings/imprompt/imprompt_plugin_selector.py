from typing import List, Optional, Set
import openai, json, re, time, os
from urllib.parse import urlparse, parse_qs
from openplugin import MessageType, PluginSelector, PluginDetected, Response, Message, \
    LLM, Plugin, \
    ToolSelectorConfig, Config, LLMProvider

plugin_prompt = """
{name_for_model}: Call this tool to get the OpenAPI spec (and usage guide) for interacting with the {name_for_model} API. 
You should only call this ONCE! 

What is the {name_for_model} API useful for? {description_for_model}.
"""

plugin_identify_prompt = """
Answer the following questions as best you can. You have access to the following tools:
{all_plugin_info_prompt}
Use the following format. Only reply with the action you want to take.:
Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of {all_plugin_names}, None if you don't want to use any tool
Begin!
Question: {prompt}
"""

plugin_operation_prompt = """
// You are an AI assistant.
// Here is a tool you can use, named {name_for_model}. The description for this plugin is: {description_for_model}.
// The Plugin rules:
// 1. Assistant ALWAYS asks user's input for ONLY the MANDATORY parameters BEFORE calling the API.
// 2. Assistant pays attention to instructions given below.
// 3. Create an HTTPS API url that represents this query.
// 4. Use this format: <HTTP VERB> <URL>
//   - An example: GET https://api.example.com/v1/products
// 5. Remove any starting periods and new lines.
// 6. Do not structure as a sentence.
// 7. Never use https://api.example.com/ in the API.

{pre_prompt}

The openapi spec file = {openapi_spec}
The instructions are: {prompt}
"""


def _extract_urls(text):
    url_pattern = re.compile(
        r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
    urls = re.findall(url_pattern, text)
    return urls


class ImpromptPluginSelector(PluginSelector):
    def __init__(
            self,
            tool_selector_config: ToolSelectorConfig,
            plugins: List[Plugin],
            config: Optional[Config],
            llm: Optional[LLM]):
        super().__init__(tool_selector_config, plugins, config, llm)
        self.llm = llm
        self.total_tokens_used = 0

    def initialize_tool_selector(
            self,
            tool_selector_config: ToolSelectorConfig,
            plugins: List[Plugin],
            config: Optional[Config],
            llm: LLM
    ):
        openai.api_key = os.environ[
            "OPENAI_API_KEY"] if config.openai_api_key is None else config.openai_api_key
        pass

    def run(self, messages: List[Message]) -> Response:
        start_test_case_time = time.time()
        plugin_operations = self.get_detected_plugin_with_operations(messages)
        response = Response(
            run_completed=True,
            final_text_response=None,
            detected_plugin_operations=plugin_operations,
            response_time=round(time.time() - start_test_case_time, 2),
            tokens_used=self.total_tokens_used,
            llm_api_cost=0
        )
        return response

    def get_detected_plugin_with_operations(self, messages: List[Message]) -> List[
        PluginDetected]:
        prompt = ""
        for message in messages:
            prompt += f"{message.message_type}: {message.content}\n"

        plugin_info_prompts = []
        plugin_names = []

        for plugin in self.plugins:
            plugin_names.append(plugin.name)
            plugin_info_prompt = plugin_prompt.format(
                name_for_model=plugin.name,
                description_for_model=plugin.description
            )
            plugin_info_prompts.append(plugin_info_prompt)
        plugin_detection_prompt = plugin_identify_prompt.format(
            all_plugin_info_prompt="".join(plugin_info_prompts),
            all_plugin_names=", ".join(plugin_names),
            prompt=prompt
        )

        response = self.run_llm_prompt(plugin_detection_prompt)
        found_plugins = []
        for line in response.get('response').splitlines():
            if line.strip().startswith("Action"):
                for val in line.split("Action:"):
                    if len(val.strip()) > 0:
                        if "-" in val:
                            val = val.split("-")[0].strip()
                        if ',' in val:
                            for v in val.split(','):
                                found_plugins.append(
                                    self.get_plugin_by_name(v.strip()))
                        else:
                            found_plugins.append(self.get_plugin_by_name(val.strip()))

        detected_plugins = []

        for plugin in found_plugins:
            if plugin is None:
                continue
            api_called = None
            mapped_operation_parameters = None
            # TODO Find a better way to find the API called
            openapi_spec_json = plugin.get_openapi_doc_json()
            formatted_plugin_operation_prompt = plugin_operation_prompt.format(
                name_for_model=plugin.name,
                description_for_model=plugin.description,
                pre_prompt=plugin.get_plugin_pre_prompts(),
                openapi_spec=json.dumps(openapi_spec_json),
                prompt=prompt
            )
            response = self.run_llm_prompt(formatted_plugin_operation_prompt)
            urls = _extract_urls(response.get('response'))
            for url in urls:
                formatted_url = url.split("?")[0].strip()
                if formatted_url in plugin.api_endpoints:
                    api_called = formatted_url
                    query_dict = parse_qs(urlparse(url).query)
                    mapped_operation_parameters = {
                        k: v[0] if type(v) == list and len(v) == 1 else v for k, v in
                        query_dict.items()}
                    break
            detected_plugins.append(PluginDetected(
                plugin=plugin,
                api_called=api_called,
                mapped_operation_parameters=mapped_operation_parameters
            ))
        return detected_plugins

    def run_llm_prompt(self, prompt):
        if self.llm.provider == LLMProvider.OpenAI:
            return self.openai_completion(prompt)
        elif self.llm.provider == LLMProvider.OpenAIChat:
            msgs = [{"role": "user", "content": prompt}]
            return self.openai_chat(msgs)
        raise ValueError(f"LLM provider {self.llm.provider} not supported")

    def run_llm(self, messages: List[Message]):
        if self.llm.provider == LLMProvider.OpenAI:
            prompt = ""
            for message in messages:
                # if message.message_type == MessageType.AIMessage or message.message_type == MessageType.SystemMessage:
                #    raise ValueError("Invalid message type, use OpenAIChat LLM")
                prompt += f"{message.message_type}: {message.content}\n"
            return self.openai_completion(prompt)
        elif self.llm.provider == LLMProvider.OpenAIChat:
            msgs = []
            for message in messages:
                if message.message_type == MessageType.HumanMessage:
                    role = "user"
                elif message.message_type == MessageType.AIMessage:
                    role = "assistant"
                elif message.message_type == MessageType.SystemMessage:
                    role = "system"
                msgs.append({
                    "role": role,
                    "content": message.content
                })
            return self.openai_chat(messages)
        raise ValueError(f"LLM provider {self.llm.provider} not supported")

    def openai_chat(self, messages):
        response = openai.ChatCompletion.create(
            model=self.llm.model_name,
            messages=messages,
            temperature=self.llm.temperature,
            max_tokens=self.llm.max_tokens,
            top_p=self.llm.top_p,
            frequency_penalty=self.llm.frequency_penalty,
            presence_penalty=self.llm.presence_penalty
        )
        self.add_to_tokens(response.get("usage").get("total_tokens"))
        return {
            "response": response.get("choices")[0].get("message").get("content"),
            "usage": response.get("usage")
        }

    def openai_completion(self, prompt):
        response = openai.Completion.create(
            model=self.llm.model_name,
            prompt=prompt,
            temperature=self.llm.temperature,
            max_tokens=self.llm.max_tokens,
            top_p=self.llm.top_p,
            frequency_penalty=self.llm.frequency_penalty,
            presence_penalty=self.llm.presence_penalty
        )
        self.add_to_tokens(response.get("usage").get("total_tokens"))
        return {
            "response": response.get("choices")[0].get("text"),
            "usage": response.get("usage")
        }

    def add_to_tokens(self, tokens):
        if tokens:
            self.total_tokens_used += tokens
